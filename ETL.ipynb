{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixando conteúdo do site dos Dados Abertos e salvando em arquivos csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# # Função para ler url e salvar conteúdo em csv \n",
    "# def fetch_and_save_in_csv(url: str, file_name: str):\n",
    "#     response = requests.get(url)\n",
    "\n",
    "#     if response.status_code == 200:\n",
    "#         with open(file_name, 'wb') as file:\n",
    "#             file.write(response.content)\n",
    "#     else:\n",
    "#         print(f'Falha ao baixar os dados. Status code: {response.status_code}')\n",
    "\n",
    "# # Lendo urls do dados abertos de urls.txt e salvando em uma lista\n",
    "# with open('urls.txt', 'r') as file:\n",
    "#     urls_list = file.readlines()\n",
    "\n",
    "# used_dir = \"./works/\"\n",
    "# name_file = \"pbf_ano\"\n",
    "\n",
    "# # Chamando a função fetch_and_save_in_csv para url em urls_list\n",
    "# for index, url in enumerate(urls_list):\n",
    "#     url = url.strip()\n",
    "#     file_name = used_dir + name_file + \"_\" + str(index) + \".csv\"\n",
    "#     if url:\n",
    "#         fetch_and_save_in_csv(url, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurando caminhos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# URL da fonte de dados\n",
    "URL = 'https://aplicacoes.mds.gov.br/sagi/servicos/misocial/?fq=anomes_s:2024*&fl=codigo_ibge%2Canomes_s%2Cqtd_familias_beneficiarias_bolsa_familia_s%2Cvalor_repassado_bolsa_familia_s%2Cpbf_vlr_medio_benef_f&fq=valor_repassado_bolsa_familia_s%3A*&q=*%3A*&rows=100000&sort=anomes_s%20desc%2C%20codigo_ibge%20asc&wt=csv'\n",
    "\n",
    "# Caminho local onde o arquivo será salvo\n",
    "workdir = './works/'\n",
    "workdir_files = os.listdir(workdir)\n",
    "full_paths = [os.path.join(workdir, file) for file in workdir_files]\n",
    "dataframes_list = []\n",
    "\n",
    "# Teste com um único arquivo\n",
    "default_file = 'bolsa-familia2024.csv'\n",
    "file_path = workdir + default_file\n",
    "\n",
    "# Caminho do driver jdbc\n",
    "jdbc_driver_path = \"/opt/trabalhos/etl-mdd/postgresql-42.7.1.jar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando JSON com informações de UF e Município"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "uf_code_path = './utils/ibge-codes/uf-code.json'\n",
    "municipios_code_path = './utils/ibge-codes/municipios-code.json'\n",
    "uf_dict, municipios_dict = {}, {}\n",
    "\n",
    "with open(uf_code_path, 'r') as file:\n",
    "    uf_dict = json.load(file)\n",
    "\n",
    "with open(municipios_code_path, 'r') as file:\n",
    "    municipios_dict = json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --user pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando sessão spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Cria a sessão spark\n",
    "spark_session = SparkSession.builder.appName('spark') \\\n",
    "                                    .config(\"spark.driver.extraClassPath\", jdbc_driver_path) \\\n",
    "                                    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "                                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo arquivo csv e montando dataframe do spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark_session.read.options(header=\"true\", delimiter=\",\", encoding=\"ISO-8859-1\", inferSchema=True).csv(file_path)\n",
    "\n",
    "for arquivo in full_paths:\n",
    "    dataframes_list.append(spark_session.read.options(header=\"true\", delimiter=\",\", encoding=\"ISO-8859-1\", inferSchema=True).csv(arquivo))\n",
    "\n",
    "column_changes = [\n",
    "    (\"ibge\", \"codigo_ibge\"),\n",
    "    (\"anomes\", \"anomes_s\"),\n",
    "    (\"qtd_familias_beneficiarias_bolsa_familia\", \"qtd_familias_beneficiarias_bolsa_familia_s\"),\n",
    "    (\"valor_repassado_bolsa_familia\", \"valor_repassado_bolsa_familia_s\")\n",
    "]\n",
    "\n",
    "### Adequando nomes de coluna\n",
    "for index, ano_csv in enumerate(dataframes_list):\n",
    "    for nome_antigo, nome_padrao in column_changes:\n",
    "        if nome_antigo in ano_csv.columns:\n",
    "            ano_csv = ano_csv.withColumnRenamed(nome_antigo, nome_padrao)\n",
    "\n",
    "    dataframes_list[index] = ano_csv\n",
    "\n",
    "for ano_csv in dataframes_list:\n",
    "    ano_csv.show()   \n",
    "\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retirado de: https://medium.com/@salibi/como-validar-o-c%C3%B3digo-de-munic%C3%ADpio-do-ibge-90dc545cc533#:~:text=O%20C%C3%B3digo%20de%20Munic%C3%ADpio%20do%20IBGE%20%C3%A9%20um%20identificador%20%C3%BAnico,o%20%C3%BAltimo%20d%C3%ADgito%2C%20um%20verificador.\n",
    "\n",
    "def last_digit_ibge(cod6: str):\n",
    "   city_exceptions = {\n",
    "                      '220191': \"2201919\",\n",
    "                      '290630': \"2202251\",\n",
    "                      '220198': \"2201988\",\n",
    "                      '261153': \"2611533\",\n",
    "                      '311783': \"3117836\",\n",
    "                      '315213': \"3152131\",\n",
    "                      '430587': \"4305871\",\n",
    "                      '520393': \"5203939\",\n",
    "                      '520396': \"5203962\",\n",
    "                      '220225': \"2202251\",\n",
    "                     }\n",
    "   \n",
    "   if cod6 in city_exceptions:\n",
    "      return city_exceptions.get(cod6)\n",
    "\n",
    "   a = int(cod6[0])\n",
    "   b = (int(cod6[1]) * 2) % 10 + (int(cod6[1]) * 2) // 10\n",
    "   c = int(cod6[2])\n",
    "   d = (int(cod6[3]) * 2) % 10 + (int(cod6[3]) * 2) // 10\n",
    "   e = int(cod6[4])\n",
    "   f = (int(cod6[5]) * 2) % 10 + (int(cod6[5]) * 2) // 10\n",
    "   digit = (10 - (a + b + c + d + e + f) % 10) % 10\n",
    "   \n",
    "   return cod6 + str(digit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando coluna de média para dados antes de 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "for index, ano_csv in enumerate(dataframes_list):\n",
    "    if 'pbf_vlr_medio_benef_f' not in ano_csv.columns:\n",
    "        valor_medio_bolsa = ano_csv.valor_repassado_bolsa_familia_s / ano_csv.qtd_familias_beneficiarias_bolsa_familia_s\n",
    "        print(valor_medio_bolsa)\n",
    "        \n",
    "        dataframes_list[index] = ano_csv.withColumn('pbf_vlr_medio_benef_f', round(valor_medio_bolsa, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para criação de colunas para UF, Município e Ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, substring, lpad\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def cria_coluna_uf(codigo_ibge):\n",
    "    return  uf_dict.get(str(codigo_ibge)[0:2])\n",
    "\n",
    "cria_coluna_uf_udf = udf(cria_coluna_uf, StringType())\n",
    "\n",
    "def cria_coluna_municipio(codigo_ibge):\n",
    "    return  municipios_dict.get(last_digit_ibge(str(codigo_ibge)))\n",
    "\n",
    "cria_coluna_municipio_udf = udf(cria_coluna_municipio, StringType())\n",
    "\n",
    "def cria_coluna_ano(anomes_s):\n",
    "    return  str(anomes_s)[0:4]\n",
    "\n",
    "cria_coluna_ano_udf = udf(cria_coluna_ano, StringType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adicionando colunas UF, Município e Ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "for index, ano_csv in enumerate(dataframes_list):\n",
    "    ano_csv = ano_csv.withColumn(\"uf\", cria_coluna_uf_udf(col(\"codigo_ibge\")))\n",
    "    ano_csv = ano_csv.withColumn(\"municipio\", cria_coluna_municipio_udf(col(\"codigo_ibge\")))\n",
    "    ano_csv = ano_csv.withColumn(\"ano\", cria_coluna_ano_udf(col(\"anomes_s\")))\n",
    "    ano_csv = ano_csv.withColumn(\"valor_repassado_bolsa_familia_s\", F.col(\"valor_repassado_bolsa_familia_s\").cast(IntegerType()))\n",
    "    ano_csv = ano_csv.withColumn('mes', substring(col('anomes_s'), 5, 2).cast('int'))\n",
    "    ano_csv = ano_csv.withColumn(\"mes_formatado\", lpad(col(\"mes\"), 2, \"0\"))\n",
    "    ano_csv = ano_csv.drop('anomes_s', 'mes')\n",
    "    ano_csv = ano_csv.withColumnRenamed('mes_formatado', 'mes')\n",
    "    \n",
    "    dataframes_list[index] = ano_csv\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unificando dataframes de diferentes anos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframe_unificado =  dataframes_list[0]\n",
    "\n",
    "for dataframe in dataframes_list[1:]:\n",
    "    dataframe_unificado = dataframe_unificado.unionAll(dataframe)\n",
    "\n",
    "dataframe_unificado.show()\n",
    "\n",
    "dataframe_unificado = dataframe_unificado.coalesce(1)\n",
    "\n",
    "output_path = \"resultado/pbf-2014-2024.csv\"\n",
    "dataframe_unificado.write.mode(\"overwrite\").csv(output_path, header=True)\n",
    "# Stop the SparkSession\n",
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurando conexão com o banco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostname_or_ip = \"localhost\"\n",
    "port = \"443\"\n",
    "db = \"misocial\"\n",
    "user = \"pbf\"\n",
    "password = \"pbf\"\n",
    "\n",
    "db_url = \"jdbc:postgresql://\" + hostname_or_ip + \":\" + port + \"/\" + db\n",
    "\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\", \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transferindo dataframe para o banco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_unificado.write.jdbc(url=db_url, table=\"pbf\", mode=\"overwrite\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desalocando sessão do spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping spark session\n",
    "spark_session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
