{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando biblioteca para download dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python311\\lib\\site-packages (2.2.0)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing\n",
      "  Using cached typing-3.7.4.3-py3-none-any.whl\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\python311\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\decas\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\decas\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.9/64.9 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "   ---------------------------------------- 0.0/164.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 164.4/164.4 kB 5.0 MB/s eta 0:00:00\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl (99 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "   ---------------------------------------- 0.0/121.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 121.4/121.4 kB ? eta 0:00:00\n",
      "Installing collected packages: urllib3, typing, idna, charset-normalizer, certifi, requests\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] O sistema não pode encontrar o arquivo especificado: 'c:\\\\Python311\\\\Scripts\\\\normalizer.exe' -> 'c:\\\\Python311\\\\Scripts\\\\normalizer.exe.deleteme'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas requests typing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixando os arquivos csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Função para ler url e salvar conteúdo em csv \n",
    "def fetch_and_save_in_csv(url: str, file_name: str):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "    else:\n",
    "        print(f'Falha ao baixar os dados. Status code: {response.status_code}')\n",
    "\n",
    "# Lendo urls do dados abertos de urls.txt e salvando em uma lista\n",
    "with open('urls.txt', 'r') as file:\n",
    "    urls_list = file.readlines()\n",
    "\n",
    "used_dir = \"./work/\"\n",
    "name_file = \"pbf_ano\"\n",
    "\n",
    "# Chamando a função fetch_and_save_in_csv para url em urls_list\n",
    "for index, url in enumerate(urls_list):\n",
    "    url = url.strip()\n",
    "    file_name = used_dir + name_file + \"_\" + str(index) + \".csv\"\n",
    "    if url:\n",
    "        fetch_and_save_in_csv(url, file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurando caminhos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Caminho local onde o arquivo será salvo\n",
    "workdir = '.\\\\work\\\\'\n",
    "\n",
    "# URL de teste\n",
    "URL = 'https://aplicacoes.mds.gov.br/sagi/servicos/misocial/?fq=anomes_s:2024*&fl=codigo_ibge%2Canomes_s%2Cqtd_familias_beneficiarias_bolsa_familia_s%2Cvalor_repassado_bolsa_familia_s%2Cpbf_vlr_medio_benef_f&fq=valor_repassado_bolsa_familia_s%3A*&q=*%3A*&rows=100000&sort=anomes_s%20desc%2C%20codigo_ibge%20asc&wt=csv'\n",
    "default_file = 'bolsa-familia2024.csv'\n",
    "file_path = workdir + default_file\n",
    "\n",
    "\n",
    "# Listando arquivos da pasta ./work/ e associando ao path da pasta\n",
    "\n",
    "workdir_files = os.listdir(workdir)\n",
    "full_paths = [os.path.join(workdir, file) for file in workdir_files]\n",
    "\n",
    "# Caminho do driver jdbc\n",
    "jdbc_driver_path = \"/opt/trabalhos/etl-mdd/postgresql-42.7.1.jar\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando JSON com informações de UF e Município"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "uf_code_path = './utils/ibge-codes/uf-code.json'\n",
    "municipios_code_path = './utils/ibge-codes/municipios-code.json'\n",
    "uf_dict, municipios_dict = {}, {}\n",
    "\n",
    "with open(uf_code_path, 'r', encoding='utf-8') as file:\n",
    "    uf_dict = json.load(file)\n",
    "\n",
    "with open(municipios_code_path, 'r', encoding='utf-8') as file:\n",
    "    municipios_dict = json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\python311\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\python311\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "%pip install --user pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando sessão spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Cria a sessão spark\n",
    "spark_session = SparkSession.builder.appName('spark') \\\n",
    "                                    .config(\"spark.driver.extraClassPath\", jdbc_driver_path) \\\n",
    "                                    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "                                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo arquivo csv e montando dataframe do spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/c:/Users/decas/Documents/tre_projects/etl-mdd/work/bolsa-familia2024.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Lista de dataframes que guardarei dados de cada arquivo\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dataframes_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mISO-8859-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arquivo \u001b[38;5;129;01min\u001b[39;00m full_paths:\n\u001b[0;32m      7\u001b[0m     dataframes_list\u001b[38;5;241m.\u001b[39mappend(spark_session\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moptions(header\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcsv(arquivo))\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/c:/Users/decas/Documents/tre_projects/etl-mdd/work/bolsa-familia2024.csv."
     ]
    }
   ],
   "source": [
    "# Lista de dataframes que guardarei dados de cada arquivo\n",
    "dataframes_list = []\n",
    "\n",
    "df = spark_session.read.options(header=\"true\", delimiter=\",\", encoding=\"ISO-8859-1\", inferSchema=True).csv(file_path)\n",
    "\n",
    "for arquivo in full_paths:\n",
    "    dataframes_list.append(spark_session.read.options(header=\"true\", delimiter=\",\", encoding=\"ISO-8859-1\", inferSchema=True).csv(arquivo))\n",
    "\n",
    "# Mudanças no nome das colunas ao longo do tempo\n",
    "column_changes = [\n",
    "    (\"ibge\", \"codigo_ibge\"),\n",
    "    (\"anomes\", \"anomes_s\"),\n",
    "    (\"qtd_familias_beneficiarias_bolsa_familia\", \"qtd_familias_beneficiarias_bolsa_familia_s\"),\n",
    "    (\"valor_repassado_bolsa_familia\", \"valor_repassado_bolsa_familia_s\")\n",
    "]\n",
    "\n",
    "### Adequando nomes de coluna\n",
    "for index, ano_csv in enumerate(dataframes_list):\n",
    "    for nome_antigo, nome_padrao in column_changes:\n",
    "        if nome_antigo in ano_csv.columns:\n",
    "            ano_csv = ano_csv.withColumnRenamed(nome_antigo, nome_padrao)\n",
    "\n",
    "    dataframes_list[index] = ano_csv\n",
    "\n",
    "for ano_csv in dataframes_list:\n",
    "    ano_csv.show()   \n",
    "\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retirado de: https://medium.com/@salibi/como-validar-o-c%C3%B3digo-de-munic%C3%ADpio-do-ibge-90dc545cc533#:~:text=O%20C%C3%B3digo%20de%20Munic%C3%ADpio%20do%20IBGE%20%C3%A9%20um%20identificador%20%C3%BAnico,o%20%C3%BAltimo%20d%C3%ADgito%2C%20um%20verificador.\n",
    "\n",
    "def last_digit_ibge(cod6: str):\n",
    "   city_exceptions = {\n",
    "                      '220191': \"2201919\",\n",
    "                      '290630': \"2202251\",\n",
    "                      '220198': \"2201988\",\n",
    "                      '261153': \"2611533\",\n",
    "                      '311783': \"3117836\",\n",
    "                      '315213': \"3152131\",\n",
    "                      '430587': \"4305871\",\n",
    "                      '520393': \"5203939\",\n",
    "                      '520396': \"5203962\",\n",
    "                     }\n",
    "   \n",
    "   if cod6 in city_exceptions:\n",
    "      return city_exceptions.get(cod6)\n",
    "\n",
    "   a = int(cod6[0])\n",
    "   b = (int(cod6[1]) * 2) % 10 + (int(cod6[1]) * 2) // 10\n",
    "   c = int(cod6[2])\n",
    "   d = (int(cod6[3]) * 2) % 10 + (int(cod6[3]) * 2) // 10\n",
    "   e = int(cod6[4])\n",
    "   f = (int(cod6[5]) * 2) % 10 + (int(cod6[5]) * 2) // 10\n",
    "   digit = (10 - (a + b + c + d + e + f) % 10) % 10\n",
    "   \n",
    "   return cod6 + str(digit)\n",
    "\n",
    "print(last_digit_ibge(\"110001\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando coluna de média para dados antes de 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, ano_csv in enumerate(dataframes_list):\n",
    "    if 'pbf_vlr_medio_benef_f' not in ano_csv.columns:\n",
    "        valor_medio_bolsa = ano_csv.valor_repassado_bolsa_familia_s / ano_csv.qtd_familias_beneficiarias_bolsa_familia_s\n",
    "        print(valor_medio_bolsa)\n",
    "        \n",
    "        dataframes_list[index] = ano_csv.withColumn('pbf_vlr_medio_benef_f', valor_medio_bolsa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando coluna para UF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def cria_coluna_uf(codigo_ibge):\n",
    "    return  uf_dict.get(str(codigo_ibge)[0:2])\n",
    "\n",
    "cria_coluna_uf_udf = udf(cria_coluna_uf, StringType())\n",
    "\n",
    "df = df.withColumn(\"uf\", cria_coluna_uf_udf(col(\"codigo_ibge\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando coluna para Município"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cria_coluna_municipio(codigo_ibge):\n",
    "    return  municipios_dict.get(last_digit_ibge(str(codigo_ibge)))\n",
    "\n",
    "cria_coluna_municipio_udf = udf(cria_coluna_municipio, StringType())\n",
    "\n",
    "df = df.withColumn(\"municipio\", cria_coluna_municipio_udf(col(\"codigo_ibge\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando coluna para Ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cria_coluna_ano(anomes_s):\n",
    "    return  str(anomes_s)[0:4]\n",
    "\n",
    "cria_coluna_ano_udf = udf(cria_coluna_ano, StringType())\n",
    "\n",
    "df = df.withColumn(\"ano\", cria_coluna_ano_udf(col(\"anomes_s\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adicionando colunas UF, Município e Ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, ano_csv in enumerate(dataframes_list):\n",
    "    ano_csv = ano_csv.withColumn(\"uf\", cria_coluna_uf_udf(col(\"codigo_ibge\")))\n",
    "    ano_csv = ano_csv.withColumn(\"municipio\", cria_coluna_municipio_udf(col(\"codigo_ibge\")))\n",
    "    ano_csv = ano_csv.withColumn(\"ano\", cria_coluna_ano_udf(col(\"anomes_s\")))\n",
    "\n",
    "    dataframes_list[index] = ano_csv\n",
    "\n",
    "for ano_csv in dataframes_list:\n",
    "    ano_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_unificado =  dataframes_list[0]\n",
    "\n",
    "for dataframe in dataframes_list[1:]:\n",
    "    dataframe_unificado = dataframe_unificado.unionAll(dataframe)\n",
    "\n",
    "dataframe_unificado.show()\n",
    "\n",
    "dataframe_unificado = dataframe_unificado.coalesce(1)\n",
    "\n",
    "output_path = \"output/unified_data.csv\"\n",
    "dataframe_unificado.write.mode(\"overwrite\").csv(output_path, header=True)\n",
    "# Stop the SparkSession\n",
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversão do dataframe para o modelo estrela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from typing import Dict, List\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "def get_columns_list_from_dimension(dimension: Dict[str, List[str]]):\n",
    "    return [col for cols in dimension for col in cols]\n",
    "\n",
    "def get_table_name_and_records(dataframe: DataFrame, dimension_table_name_and_columns: Dict[str, List[str]]) -> List[tuple[str, DataFrame]]:\n",
    "    dimensions = []\n",
    "\n",
    "    for dimension_table_name, dimension_columns in dimension_table_name_and_columns:\n",
    "        dimension_records = dataframe.select(*dimension_columns).distinct()\n",
    "        surrogate_key_column_name = f\"sk_{dimension_table_name.replace('dim_', '')}\"\n",
    "\n",
    "        # add unique and increasing id to dimension (but not consecutive)\n",
    "        unique_and_increasing_id = F.monotonically_increasing_id()\n",
    "        dimension_records = dimension_records.withColumn(\n",
    "            surrogate_key_column_name,\n",
    "            unique_and_increasing_id\n",
    "        )\n",
    "\n",
    "        dimension_table_in_tuple = (dimension_table_name, dimension_records)\n",
    "\n",
    "        dimensions.append(dimension_table_in_tuple)\n",
    "    \n",
    "    return dimensions\n",
    "\n",
    "\n",
    "def transform_spark_dataframe_into_star_schema(\n",
    "    original_dataframe: DataFrame,\n",
    "    fact_columns: List[str]  = [\"col1\", \"col2\"],\n",
    "    fact_table_name = \"tabela_fato\",\n",
    "    mapping_dimension_columns: Dict[str, List[str]] = {'dim1':[\"col3\", \"col4\"], \"dim2\":[\"col5\", \"col6\"]},\n",
    "):\n",
    "    dimension_columns_separated_by_dimension = mapping_dimension_columns.values()\n",
    "\n",
    "    dimension_columns = get_columns_list_from_dimension(dimension_columns_separated_by_dimension)\n",
    "\n",
    "    columns_from_fact_and_dimension = fact_columns + dimension_columns\n",
    "\n",
    "    original_dataframe = original_dataframe.select(*columns_from_fact_and_dimension)\n",
    "\n",
    "    dimension_table_name_and_columns = mapping_dimension_columns.items()\n",
    "\n",
    "    dimensions = get_table_name_and_records(original_dataframe, dimension_table_name_and_columns)\n",
    "\n",
    "    # Substitui as colunas de dimensão pelo respectivo SK na tabela fato\n",
    "    # ------------------------------------------------------------------\n",
    "    for table_name, records in dimensions:\n",
    "        # join the dimension dataframe to the original dataframe\n",
    "        dimension_columns_by_dimension_from_dataframe = [\n",
    "            original_dataframe[column] == records[column]\n",
    "            for column in mapping_dimension_columns[table_name]\n",
    "        ]\n",
    "        \n",
    "        original_dataframe = original_dataframe.join(\n",
    "            F.broadcast(records), \n",
    "            on=dimension_columns_by_dimension_from_dataframe,\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "    # drop the original columns\n",
    "    original_dataframe = original_dataframe.drop(*dimension_columns)\n",
    "\n",
    "    fact_table = (fact_table_name, original_dataframe)\n",
    "    \n",
    "    return dimensions + [fact_table]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_schema = transform_spark_dataframe_into_star_schema(\n",
    "    selected_columns_df,\n",
    "    fact_columns=[\"QT_VOTOS_NOMINAIS_VALIDOS\", \"QT_VOTOS_NOMINAIS\"],\n",
    "    fact_table_name=\"tabela_fato\",\n",
    "    mapping_dimension_columns={\n",
    "        'dim_municipio': [\"SG_UF\", \"NM_MUNICIPIO\"],\n",
    "        'dim_cargo': [\"DS_CARGO\"],\n",
    "        'dim_ds_eleicao':[\"DS_ELEICAO\"],\n",
    "        'dim_partido':[\"SG_PARTIDO\",\"NM_PARTIDO\", \"NR_PARTIDO\"],\n",
    "        'dim_candidato':[\"NM_CANDIDATO\", \"NR_CANDIDATO\", \"NM_URNA_CANDIDATO\"],\n",
    "        'dim_turno':[\"NR_TURNO\"],\n",
    "        'dim_tp_agrangencia':[\"TP_ABRANGENCIA\"],\n",
    "        'dim_zona':[\"NR_ZONA\"],\n",
    "        'dim_situacao_candidatura':[\"DS_SITUACAO_CANDIDATURA\"],\n",
    "        'dim_coligacao':[\"NM_COLIGACAO\", \"DS_COMPOSICAO_COLIGACAO\"],\n",
    "        \"dim_voto_transito\":[\"ST_VOTO_EM_TRANSITO\"],\n",
    "        'dim_situacaof_turno':[\"DS_SIT_TOT_TURNO\"],\n",
    "        'dim_destinacao_voto':[\"NM_TIPO_DESTINACAO_VOTOS\"]\n",
    "    },   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurando conexão com o banco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostname_or_ip = \"34.172.175.190\"\n",
    "port = \"443\"\n",
    "db = \"metabase\"\n",
    "user = \"star\"\n",
    "password = \"star\"\n",
    "\n",
    "db_url = \"jdbc:postgresql://\" + hostname_or_ip + \":\" + port + \"/\" + db\n",
    "\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\", \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transferindo modelo estrela para o banco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in star_schema:\n",
    "    table_name,dataframe = item\n",
    "    print(f\"Writing {table_name} to Eleicoes DB\")\n",
    "    if table_name == \"dim_municipio\":\n",
    "        dataframe.write.jdbc(url=db_url, table=table_name, mode=\"overwrite\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desalocando sessão do spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping spark session\n",
    "spark_session.stop()\n",
    "\n",
    "# Cleaning up files \n",
    "# Delete the directory and all its contents\n",
    "# import shutil\n",
    "\n",
    "# shutil.rmtree(workdir+'extracted/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
