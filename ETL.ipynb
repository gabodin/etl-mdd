{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurando caminhos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL da fonte de dados\n",
    "URL = 'https://aplicacoes.mds.gov.br/sagi/servicos/misocial/?fq=anomes_s:2024*&fl=codigo_ibge%2Canomes_s%2Cqtd_familias_beneficiarias_bolsa_familia_s%2Cvalor_repassado_bolsa_familia_s%2Cpbf_vlr_medio_benef_f&fq=valor_repassado_bolsa_familia_s%3A*&q=*%3A*&rows=100000&sort=anomes_s%20desc%2C%20codigo_ibge%20asc&wt=csv'\n",
    "# Caminho local onde o arquivo será salvo\n",
    "workdir = './work/'\n",
    "file = 'bolsa-familia-2024.csv'\n",
    "# Caminho do driver jdbc\n",
    "jdbc_driver_path = \"/opt/trabalhos/etl-mdd/postgresql-42.7.1.jar\"\n",
    "file_path = workdir + file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando JSON com informações de UF e Município"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'12': 'Acre', '27': 'Alagoas', '16': 'Amapá', '13': 'Amazonas', '29': 'Bahia', '23': 'Ceará', '53': 'Distrito Federal', '32': 'Espírito Santo', '52': 'Goiás', '21': 'Maranhão', '51': 'Mato Grosso', '50': 'Mato Grosso do Sul', '31': 'Minas Gerais', '15': 'Pará', '25': 'Paraíba', '41': 'Paraná', '26': 'Pernambuco', '22': 'Piauí', '24': 'Rio Grande do Norte', '43': 'Rio Grande do Sul', '33': 'Rio de Janeiro', '11': 'Rondônia', '14': 'Roraima', '42': 'Santa Catarina', '35': 'São Paulo', '28': 'Sergipe', '17': 'Tocantins'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_file_path = './utils/uf-code.json'\n",
    "uf_dict = {}\n",
    "\n",
    "with open(json_file_path, 'r') as file:\n",
    "    uf_dict = json.load(file)\n",
    "\n",
    "print(uf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/gabodin/.local/lib/python3.10/site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/gabodin/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando sessão spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/06/17 19:48:28 WARN Utils: Your hostname, RRNWRESID05 resolves to a loopback address: 127.0.1.1; using 172.23.195.127 instead (on interface eth0)\n",
      "24/06/17 19:48:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/17 19:48:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Cria a sessão spark\n",
    "spark_session = SparkSession.builder.appName('spark') \\\n",
    "                                    .config(\"spark.driver.extraClassPath\", jdbc_driver_path) \\\n",
    "                                    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "                                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo arquivo csv e montando dataframe do spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.read.options(header=\"true\", delimiter=\",\", encoding=\"ISO-8859-1\", inferSchema=True).csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retirado de: https://medium.com/@salibi/como-validar-o-c%C3%B3digo-de-munic%C3%ADpio-do-ibge-90dc545cc533#:~:text=O%20C%C3%B3digo%20de%20Munic%C3%ADpio%20do%20IBGE%20%C3%A9%20um%20identificador%20%C3%BAnico,o%20%C3%BAltimo%20d%C3%ADgito%2C%20um%20verificador.\n",
    "\n",
    "def last_digit_ibge(cod6: str):\n",
    "   a = int(cod6[0])\n",
    "   b = (int(cod6[1]) * 2) % 10 + (int(cod6[1]) * 2) // 10\n",
    "   c = int(cod6[2])\n",
    "   d = (int(cod6[3]) * 2) % 10 + (int(cod6[3]) * 2) // 10\n",
    "   e = int(cod6[4])\n",
    "   f = (int(cod6[5]) * 2) % 10 + (int(cod6[5]) * 2) // 10\n",
    "   digit = (10 - (a + b + c + d + e + f) % 10) % 10\n",
    "   return str(digit)\n",
    "\n",
    "# city_code = \"355030\"\n",
    "# result = city_code + last_digit_ibge(city_code)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprimindo primeiras linhas do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------------------------------------------+--------------------------------+----------------------+\n",
      "|codigo_ibge| anomes_s| qtd_familias_beneficiarias_bolsa_familia_s| valor_repassado_bolsa_familia_s| pbf_vlr_medio_benef_f|\n",
      "+-----------+---------+-------------------------------------------+--------------------------------+----------------------+\n",
      "|     110001| 202406.0|                                     1705.0|                       1176837.0|                690.63|\n",
      "|     110002| 202406.0|                                     7494.0|                       5117672.0|                683.82|\n",
      "|     110003| 202406.0|                                      323.0|                        214990.0|                 665.6|\n",
      "|     110004| 202406.0|                                     6105.0|                       4127413.0|                676.18|\n",
      "|     110005| 202406.0|                                     1203.0|                        862839.0|                718.43|\n",
      "|     110006| 202406.0|                                      492.0|                        340501.0|                692.08|\n",
      "|     110007| 202406.0|                                      330.0|                        213355.0|                646.53|\n",
      "|     110008| 202406.0|                                     2123.0|                       1530740.0|                721.71|\n",
      "|     110009| 202406.0|                                     1774.0|                       1173571.0|                662.29|\n",
      "|     110010| 202406.0|                                     5038.0|                       3904677.0|                776.28|\n",
      "|     110011| 202406.0|                                     3560.0|                       2361603.0|                664.49|\n",
      "|     110012| 202406.0|                                     7676.0|                       5264932.0|                686.34|\n",
      "|     110013| 202406.0|                                     2781.0|                       1898136.0|                682.54|\n",
      "|     110014| 202406.0|                                     1599.0|                       1028141.0|                642.99|\n",
      "|     110015| 202406.0|                                     2475.0|                       1675150.0|                677.92|\n",
      "|     110018| 202406.0|                                     1645.0|                       1100733.0|                669.55|\n",
      "|     110020| 202406.0|                                    40892.0|                     2.8563667E7|                698.98|\n",
      "|     110025| 202406.0|                                     2246.0|                       1488779.0|                662.86|\n",
      "|     110026| 202406.0|                                      228.0|                        160052.0|                701.98|\n",
      "|     110028| 202406.0|                                     2110.0|                       1409088.0|                668.13|\n",
      "+-----------+---------+-------------------------------------------+--------------------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first_column_df = df.select(\"codigo_ibge\")\n",
    "# first_column_df.show()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando coluna para UF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/17 19:48:41 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def categorize_age(uf):\n",
    "    return  uf_dict.get(str(uf)[0:2]) \n",
    "\n",
    "categorize_age_udf = udf(categorize_age, StringType())\n",
    "\n",
    "df = df.withColumn(\"uf\", categorize_age_udf(col(\"codigo_ibge\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------------------------------------------+--------------------------------+----------------------+--------+\n",
      "|codigo_ibge| anomes_s| qtd_familias_beneficiarias_bolsa_familia_s| valor_repassado_bolsa_familia_s| pbf_vlr_medio_benef_f|      uf|\n",
      "+-----------+---------+-------------------------------------------+--------------------------------+----------------------+--------+\n",
      "|     110001| 202406.0|                                     1705.0|                       1176837.0|                690.63|Rondônia|\n",
      "|     110002| 202406.0|                                     7494.0|                       5117672.0|                683.82|Rondônia|\n",
      "|     110003| 202406.0|                                      323.0|                        214990.0|                 665.6|Rondônia|\n",
      "|     110004| 202406.0|                                     6105.0|                       4127413.0|                676.18|Rondônia|\n",
      "|     110005| 202406.0|                                     1203.0|                        862839.0|                718.43|Rondônia|\n",
      "|     110006| 202406.0|                                      492.0|                        340501.0|                692.08|Rondônia|\n",
      "|     110007| 202406.0|                                      330.0|                        213355.0|                646.53|Rondônia|\n",
      "|     110008| 202406.0|                                     2123.0|                       1530740.0|                721.71|Rondônia|\n",
      "|     110009| 202406.0|                                     1774.0|                       1173571.0|                662.29|Rondônia|\n",
      "|     110010| 202406.0|                                     5038.0|                       3904677.0|                776.28|Rondônia|\n",
      "|     110011| 202406.0|                                     3560.0|                       2361603.0|                664.49|Rondônia|\n",
      "|     110012| 202406.0|                                     7676.0|                       5264932.0|                686.34|Rondônia|\n",
      "|     110013| 202406.0|                                     2781.0|                       1898136.0|                682.54|Rondônia|\n",
      "|     110014| 202406.0|                                     1599.0|                       1028141.0|                642.99|Rondônia|\n",
      "|     110015| 202406.0|                                     2475.0|                       1675150.0|                677.92|Rondônia|\n",
      "|     110018| 202406.0|                                     1645.0|                       1100733.0|                669.55|Rondônia|\n",
      "|     110020| 202406.0|                                    40892.0|                     2.8563667E7|                698.98|Rondônia|\n",
      "|     110025| 202406.0|                                     2246.0|                       1488779.0|                662.86|Rondônia|\n",
      "|     110026| 202406.0|                                      228.0|                        160052.0|                701.98|Rondônia|\n",
      "|     110028| 202406.0|                                     2110.0|                       1409088.0|                668.13|Rondônia|\n",
      "+-----------+---------+-------------------------------------------+--------------------------------+----------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversão do dataframe para o modelo estrela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from typing import Dict, List\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "def get_columns_list_from_dimension(dimension: Dict[str, List[str]]):\n",
    "    return [col for cols in dimension for col in cols]\n",
    "\n",
    "def get_table_name_and_records(dataframe: DataFrame, dimension_table_name_and_columns: Dict[str, List[str]]) -> List[tuple[str, DataFrame]]:\n",
    "    dimensions = []\n",
    "\n",
    "    for dimension_table_name, dimension_columns in dimension_table_name_and_columns:\n",
    "        dimension_records = dataframe.select(*dimension_columns).distinct()\n",
    "        surrogate_key_column_name = f\"sk_{dimension_table_name.replace('dim_', '')}\"\n",
    "\n",
    "        # add unique and increasing id to dimension (but not consecutive)\n",
    "        unique_and_increasing_id = F.monotonically_increasing_id()\n",
    "        dimension_records = dimension_records.withColumn(\n",
    "            surrogate_key_column_name,\n",
    "            unique_and_increasing_id\n",
    "        )\n",
    "\n",
    "        dimension_table_in_tuple = (dimension_table_name, dimension_records)\n",
    "\n",
    "        dimensions.append(dimension_table_in_tuple)\n",
    "    \n",
    "    return dimensions\n",
    "\n",
    "\n",
    "def transform_spark_dataframe_into_star_schema(\n",
    "    original_dataframe: DataFrame,\n",
    "    fact_columns: List[str]  = [\"col1\", \"col2\"],\n",
    "    fact_table_name = \"tabela_fato\",\n",
    "    mapping_dimension_columns: Dict[str, List[str]] = {'dim1':[\"col3\", \"col4\"], \"dim2\":[\"col5\", \"col6\"]},\n",
    "):\n",
    "    dimension_columns_separated_by_dimension = mapping_dimension_columns.values()\n",
    "\n",
    "    dimension_columns = get_columns_list_from_dimension(dimension_columns_separated_by_dimension)\n",
    "\n",
    "    columns_from_fact_and_dimension = fact_columns + dimension_columns\n",
    "\n",
    "    original_dataframe = original_dataframe.select(*columns_from_fact_and_dimension)\n",
    "\n",
    "    dimension_table_name_and_columns = mapping_dimension_columns.items()\n",
    "\n",
    "    dimensions = get_table_name_and_records(original_dataframe, dimension_table_name_and_columns)\n",
    "\n",
    "    # Substitui as colunas de dimensão pelo respectivo SK na tabela fato\n",
    "    # ------------------------------------------------------------------\n",
    "    for table_name, records in dimensions:\n",
    "        # join the dimension dataframe to the original dataframe\n",
    "        dimension_columns_by_dimension_from_dataframe = [\n",
    "            original_dataframe[column] == records[column]\n",
    "            for column in mapping_dimension_columns[table_name]\n",
    "        ]\n",
    "        \n",
    "        original_dataframe = original_dataframe.join(\n",
    "            F.broadcast(records), \n",
    "            on=dimension_columns_by_dimension_from_dataframe,\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "    # drop the original columns\n",
    "    original_dataframe = original_dataframe.drop(*dimension_columns)\n",
    "\n",
    "    fact_table = (fact_table_name, original_dataframe)\n",
    "    \n",
    "    return dimensions + [fact_table]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_schema = transform_spark_dataframe_into_star_schema(\n",
    "    selected_columns_df,\n",
    "    fact_columns=[\"QT_VOTOS_NOMINAIS_VALIDOS\", \"QT_VOTOS_NOMINAIS\"],\n",
    "    fact_table_name=\"tabela_fato\",\n",
    "    mapping_dimension_columns={\n",
    "        'dim_municipio': [\"SG_UF\", \"NM_MUNICIPIO\"],\n",
    "        'dim_cargo': [\"DS_CARGO\"],\n",
    "        'dim_ds_eleicao':[\"DS_ELEICAO\"],\n",
    "        'dim_partido':[\"SG_PARTIDO\",\"NM_PARTIDO\", \"NR_PARTIDO\"],\n",
    "        'dim_candidato':[\"NM_CANDIDATO\", \"NR_CANDIDATO\", \"NM_URNA_CANDIDATO\"],\n",
    "        'dim_turno':[\"NR_TURNO\"],\n",
    "        'dim_tp_agrangencia':[\"TP_ABRANGENCIA\"],\n",
    "        'dim_zona':[\"NR_ZONA\"],\n",
    "        'dim_situacao_candidatura':[\"DS_SITUACAO_CANDIDATURA\"],\n",
    "        'dim_coligacao':[\"NM_COLIGACAO\", \"DS_COMPOSICAO_COLIGACAO\"],\n",
    "        \"dim_voto_transito\":[\"ST_VOTO_EM_TRANSITO\"],\n",
    "        'dim_situacaof_turno':[\"DS_SIT_TOT_TURNO\"],\n",
    "        'dim_destinacao_voto':[\"NM_TIPO_DESTINACAO_VOTOS\"]\n",
    "    },   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurando conexão com o banco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostname_or_ip = \"34.172.175.190\"\n",
    "port = \"443\"\n",
    "db = \"metabase\"\n",
    "user = \"star\"\n",
    "password = \"star\"\n",
    "\n",
    "db_url = \"jdbc:postgresql://\" + hostname_or_ip + \":\" + port + \"/\" + db\n",
    "\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\", \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transferindo modelo estrela para o banco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in star_schema:\n",
    "    table_name,dataframe = item\n",
    "    print(f\"Writing {table_name} to Eleicoes DB\")\n",
    "    if table_name == \"dim_municipio\":\n",
    "        dataframe.write.jdbc(url=db_url, table=table_name, mode=\"overwrite\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desalocando sessão do spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping spark session\n",
    "spark_session.stop()\n",
    "\n",
    "# Cleaning up files \n",
    "# Delete the directory and all its contents\n",
    "# import shutil\n",
    "\n",
    "# shutil.rmtree(workdir+'extracted/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
